# LLM Service 

Run your LLM service Locally

## how to run it locally

* First download gguf from https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF , `llama-2-13b-chat.Q5_K_S.gguf`
* move this modal to `backend/statics/llama-2-13b-chat.Q5_K_S.gguf`
* 