# LLM Service 

Run your LLM service Locally

## how to run it locally

* First download gguf from https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF , `llama-2-13b-chat.Q5_K_S.gguf`
* move this model to `backend/src/main/resources/llama-2-13b-chat.Q5_K_S.gguf`